{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# __Scraping Restaurants in Delhi From EazyDiner__\n", "\n", "***\n", "\n", "![](https://i.imgur.com/ZAl12g3.png)\n", "\n", "***\n", "\n", "[EazyDiner](https://www.eazydiner.com/static/about-us) provides a guide to eating out that offers insider tips, discount, exclusive and expert reviews by top critics. The platform has all the answers for the most enjoyable, authentic and friction-free table booking experience, with over 10,000 restaurants in over 150 cities in India & Dubai.\n", "\n", "***\n", "\n", "![Imgur](https://i.imgur.com/CC2FfCF.png)\n", "\n", "It also hosts list of top restaurants in a city and we will be using this list to scrape the top restaurants in the region [Delhi-NCR](https://www.eazydiner.com/restaurants?location=delhi-ncr&pax=2&total=281&page=1) using [_web scraping_](https://www.geeksforgeeks.org/what-is-web-scraping-and-how-to-use-it/), an automatic methord of obtaining large amounts of data from websites using coding languages such as `Python`,`C++`,`Node.js`,`Ruby`,`PHP` etc. \n", "\n", "For this project we will be using Python libraries [requests](https://pypi.org/project/requests/) and [Beautifulsoup4](https://pypi.org/project/beautifulsoup4/) for scraping data from this page.\n", "\n", "***\n", "\n", "#### Here is an outline of the steps we will follow\n", "\n", "##### 1. Setting up the environment\n", "##### 2.  Downloading the page using `requests` & parsing it with `BeautifulSoup` \n", "##### 3. Extracting restaurant information & appending to a dictionary\n", "##### 4. Compiling the data from multiple pages into a single file using lists and dictionaries\n", "##### 5. Exporting the data to a .CSV file\n", "\n", "***\n", "\n", "![Imgur](https://i.imgur.com/KwwD5KA.png)\n", "\n", "__By the end of the project we would have created a .CSV file in this format__\n", "\n", "***\n", "\n", "\n", "Use the \"Run\" button to execute the code."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Setting up the environment\n", "\n", "We will be using the `requests` ,`BeautifulSoup` and `pandas` libraries in this project"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# Requests will help us with fetching the HTML page from a website.\n", "\n", "import requests\n", "\n", "# Next,we will use BeautifulSoup to process the HTML formated text file for data extraction.\n", "\n", "from bs4 import BeautifulSoup\n", "\n", "# After the data has been parsed and stored, we will use pandas to extract it into a '.csv' file\n", "\n", "import pandas as pd\n", "\n", "#\n", "import jovian"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__The environment is all set up now and we can call any function from these libraries.__\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Downloading the page using `requests` & parsing it with `BeautifulSoup` \n", "\n", "Here we will define function get_page() that takes a URL as input and with the help of `requests` & `BeautifulSoup` returns a BS4 doc."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["def get_page(url):\n", "    \n", "    # requests.get returns a response object containing the data from the web page.\n", "    response = requests.get(url)\n", "    \n", "    # status_code is used to check if the request was successful and if it's not then we will raise an exception.\n", "    if response.status_code != 200:\n", "        \n", "        # Exception will be raised if the status code is not 200\n", "        raise Exception (\"Unable to fetch page \" + url)\n", "    \n", "    # At the end of function it will return a beautifulsoup doc\n", "    return BeautifulSoup(response.text,'html.parser')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Now the function can be called using `get_page(xyz.com)` and it will return a beautifulsoup doc.__\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Extracting restaurant information & Writing it in a dictionary\n", "\n", "##### Here we will extract the data from the restaurant listings & append it to a dictionary \n", "Restaurant Name, Restaurant Location, Cost for two, Cuisines, Restaurant Rating, Restaurant Image, Link to restaurant page\n", "\n", "\n", "***\n", "__Inspect Element Page__\n", "![Imgur](https://i.imgur.com/mNCLTg9.png)\n", "\n", "__For parsing data from a HTML page using beautiful soup we will need the [CSS selectors](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors) of the elements, to get these selectors we will use the [inspect element](https://devmountain.com/blog/how-to-use-inspect-element-jump-into-what-makes-a-web-page-tick/) function available in the web browser.__\n", "\n", "***\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Restaurant Listings\n", "\n", "![Imgur](https://i.imgur.com/5zO4cT6.png)\n", "\n", "The restaurant listings are inside a `div` tag with `padding-10 radius-4 bg-white restaurant margin-b-10` class"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["def get_restaurant_listings(doc):\n", "    ''''''\n", "    # Declaring a variable selector that contains class for name tag.\n", "    selector = 'padding-10 radius-4 bg-white restaurant margin-b-10'\n", "    \n", "    # Returning the restaurant lising tags\n", "    return  doc.find_all('div',class_=selector)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Extract Restaurant Details\n", "\n", "Similarly by using the inspect element we can find tags for all the fields"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "    \n", "`restaurant_name = listing.find('h3',class_='grey res_name font-20 bold inline-block')`\n", "\n", "`restaurant_location = listing.find('h3',class_='margin-t-5 res_loc')`\n", "\n", "`cost_for_2 = listing.find('span',class_='padding-l-10 grey cost_for_two')`\n", "\n", "`cusisine = listing.find('div',class_='grey padding-l-10 res_cuisine')`\n", "\n", "`rating = listing.find('span',class_='critic')`\n", "\n", "`restaurant_img = listing.find('img',class_='radius-4 res_name lazy')`\n", "\n", "`restaurant_href = listing.find('a',class_='btn btn-primary height-40 block bold padding-10 font-14 apxor_click')`"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# Parsing all required data from the listing \n", "\n", "def get_restaurant_data(listing,info,base_url):\n", "    restaurant_name = listing.find('h3',class_='grey res_name font-20 bold inline-block')\n", "    restaurant_location = listing.find('h3',class_='margin-t-5 res_loc')\n", "    cost_for_2 = listing.find('span',class_='padding-l-10 grey cost_for_two')\n", "    cusisine = listing.find('div',class_='grey padding-l-10 res_cuisine')\n", "    rating = listing.find('span',class_='critic')\n", "    restaurant_img = listing.find('img',class_='radius-4 res_name lazy')\n", "    restaurant_href = listing.find('a',class_='btn btn-primary height-40 block bold padding-10 font-14 apxor_click')\n", "    \n", "    return append_data(info,restaurant_name,restaurant_location,cost_for_2,cusisine,rating,restaurant_href,restaurant_img)\n"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# Appending all parsed data to a dictionary\n", "\n", "def append_data(info,restaurant_name,restaurant_location,cost_for_2,cusisine,rating,restaurant_href,restaurant_img):\n", "    base_url = \"https://www.eazydiner.com\" \n", "    info['restaurant_names'].append(restaurant_name.text.strip() if restaurant_name else 'N/A')\n", "    info['restaurant_locations'].append(restaurant_location.text.strip() if restaurant_location else 'N/A')\n", "    info['costs_for_2'].append(cost_for_2.text[:-7] if cost_for_2 else 'N/A')\n", "    info['cusisines'].append(cusisine.text.strip() if cusisine else 'N/A')\n", "    info['ratings'].append(rating.text.strip() if rating else 'N/A')\n", "    info['restaurant_imgs'].append(restaurant_img['data-src'] if restaurant_img else 'N/A')\n", "    info['restaurant_pages'].append(base_url+restaurant_href['href'] if restaurant_href else 'N/A')\n", "    \n", "    return info"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__We have defined 3 functions `get_restaurant_listings`, `append_data` & `get_restaurant_data` , first for getting listing tags form the page and second for extracting data from the listing and third to append it to a dictionary.__\n", "***"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compiling the data from multiple pages into a single file using lists and dictionaries.\n", "\n", "`intialize_dictionary()` will be used create a empty dictionary for every the data is scraped\n", "\n", "`page_parser()` to get the data from every listing on the page\n", "\n", "`website_scraper()` to get every page on the website"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def intialize_dictionary():\n", "    \n", "    # Intitalizing a new dictionary for stong the values\n", "    info = {\n", "        'restaurant_names':[],\n", "        'restaurant_locations':[],\n", "        'costs_for_2':[],\n", "        'cusisines':[],\n", "        'ratings':[],\n", "        'restaurant_imgs': [],\n", "        'restaurant_pages':[]\n", "    }\n", "              \n", "    return info"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["def page_parser(x,max_page,website_page_base_url,info,base_url):\n", "    \n", "    while True: # Keep the loop running till a break condition is met\n", "        \n", "        # Adding if condition to stop the function if the requested number of pages have been scraped\n", "        if x > int(max_page):         \n", "            print(\"Process completed!, No more data to scrape after page {}\".format(x-1)) # printing a confirmation message\n", "            break\n", "        \n", "        page_url = website_page_base_url + str(x) # Creating page url\n", "        print(\"Scraping Page {}.\".format(x))  # printing a confirmation message\n", "        doc = get_page(page_url) # Calling function to get bs4 doc\n", "        doc_listings = get_restaurant_listings(doc) # geting all the listing on the page\n", "        \n", "        if len(doc_listings) < 1: # Stop the loop if no data to scrape\n", "            print(\"No more listings left to scrape, pages scraped successfully {}\".format(x-1)) # printing a confirmation message\n", "            break\n", "        \n", "        # Starting a for loop to get data from page\n", "        for listing in doc_listings:           \n", "            info = get_restaurant_data(listing,info,base_url) # Extracting data from all listing using a for loop\n", "\n", "        print(\"{} listings scraped\".format(len(info['restaurant_names']))) # printing a confirmation \n", "        print(\"Page {} completed \\n\".format(x)) # printing a confirmation \n", "        \n", "        # Increasing the page number\n", "        x = x + 1\n", "        \n", "    return info"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"code_folding": []}, "outputs": [], "source": ["def website_scraper():\n", "    \n", "    # Assigning values to variables that will be used in the function\n", "    x = 1 # x is the starting page number\n", "    base_url = \"https://www.eazydiner.com\"  \n", "    website_page_base_url = base_url + '/restaurants?location=delhi-ncr&pax=2&total=281&page='\n", "    \n", "    # Intitalizing a new dictionary for stong the values\n", "    info = intialize_dictionary()\n", "    print(\"Initializing a new database \\n\")\n", "        \n", "    # Asking user for input\n", "    max_page = input(\"Please enter the number of pages you want to scrape: \")\n", "    \n", "    # Calling previously defined function to get the data from the page\n", "    info = page_parser(x,max_page,website_page_base_url,info,base_url)\n", "    \n", "    return info"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exporting the data to a .CSV file\n", "\n", "After parsing the data from the web page and storing it in a dictionary , we will use `pandas` to export it to a .csv file"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["def get_restaurant_csv():\n", "    \n", "    # Variable ourput calls the function get_restaurants() and stores it's value inside it.\n", "    output = website_scraper()\n", "    \n", "    # Convering the dictionary outpt to a pandas dataframe\n", "    df = pd.DataFrame(output)\n", "    \n", "    # Adding current date \n", "    \n", "    # returning restaurants.csv file\n", "    return df.to_csv('restaurants.csv', index=None),print(\"Task Completed\")"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Initializing a new database \n", "\n", "Please enter the number of pages you want to scrape: 1000\n", "Scraping Page 1.\n", "18 listings scraped\n", "Page 1 completed \n", "\n", "Scraping Page 2.\n", "36 listings scraped\n", "Page 2 completed \n", "\n", "Scraping Page 3.\n", "54 listings scraped\n", "Page 3 completed \n", "\n", "Scraping Page 4.\n", "72 listings scraped\n", "Page 4 completed \n", "\n", "Scraping Page 5.\n", "90 listings scraped\n", "Page 5 completed \n", "\n", "Scraping Page 6.\n", "108 listings scraped\n", "Page 6 completed \n", "\n", "Scraping Page 7.\n", "126 listings scraped\n", "Page 7 completed \n", "\n", "Scraping Page 8.\n", "144 listings scraped\n", "Page 8 completed \n", "\n", "Scraping Page 9.\n", "162 listings scraped\n", "Page 9 completed \n", "\n", "Scraping Page 10.\n", "180 listings scraped\n", "Page 10 completed \n", "\n", "Scraping Page 11.\n", "198 listings scraped\n", "Page 11 completed \n", "\n", "Scraping Page 12.\n", "216 listings scraped\n", "Page 12 completed \n", "\n", "Scraping Page 13.\n", "234 listings scraped\n", "Page 13 completed \n", "\n", "Scraping Page 14.\n", "252 listings scraped\n", "Page 14 completed \n", "\n", "Scraping Page 15.\n", "270 listings scraped\n", "Page 15 completed \n", "\n", "Scraping Page 16.\n", "288 listings scraped\n", "Page 16 completed \n", "\n", "Scraping Page 17.\n", "306 listings scraped\n", "Page 17 completed \n", "\n", "Scraping Page 18.\n", "324 listings scraped\n", "Page 18 completed \n", "\n", "Scraping Page 19.\n", "342 listings scraped\n", "Page 19 completed \n", "\n", "Scraping Page 20.\n", "360 listings scraped\n", "Page 20 completed \n", "\n", "Scraping Page 21.\n", "378 listings scraped\n", "Page 21 completed \n", "\n", "Scraping Page 22.\n", "396 listings scraped\n", "Page 22 completed \n", "\n", "Scraping Page 23.\n", "414 listings scraped\n", "Page 23 completed \n", "\n", "Scraping Page 24.\n", "432 listings scraped\n", "Page 24 completed \n", "\n", "Scraping Page 25.\n", "450 listings scraped\n", "Page 25 completed \n", "\n", "Scraping Page 26.\n", "468 listings scraped\n", "Page 26 completed \n", "\n", "Scraping Page 27.\n", "486 listings scraped\n", "Page 27 completed \n", "\n", "Scraping Page 28.\n", "504 listings scraped\n", "Page 28 completed \n", "\n", "Scraping Page 29.\n", "522 listings scraped\n", "Page 29 completed \n", "\n", "Scraping Page 30.\n", "540 listings scraped\n", "Page 30 completed \n", "\n", "Scraping Page 31.\n", "558 listings scraped\n", "Page 31 completed \n", "\n", "Scraping Page 32.\n", "576 listings scraped\n", "Page 32 completed \n", "\n", "Scraping Page 33.\n", "594 listings scraped\n", "Page 33 completed \n", "\n", "Scraping Page 34.\n", "612 listings scraped\n", "Page 34 completed \n", "\n", "Scraping Page 35.\n", "630 listings scraped\n", "Page 35 completed \n", "\n", "Scraping Page 36.\n", "648 listings scraped\n", "Page 36 completed \n", "\n", "Scraping Page 37.\n", "666 listings scraped\n", "Page 37 completed \n", "\n", "Scraping Page 38.\n", "684 listings scraped\n", "Page 38 completed \n", "\n", "Scraping Page 39.\n", "702 listings scraped\n", "Page 39 completed \n", "\n", "Scraping Page 40.\n", "720 listings scraped\n", "Page 40 completed \n", "\n", "Scraping Page 41.\n", "738 listings scraped\n", "Page 41 completed \n", "\n", "Scraping Page 42.\n", "756 listings scraped\n", "Page 42 completed \n", "\n", "Scraping Page 43.\n", "774 listings scraped\n", "Page 43 completed \n", "\n", "Scraping Page 44.\n", "792 listings scraped\n", "Page 44 completed \n", "\n", "Scraping Page 45.\n", "810 listings scraped\n", "Page 45 completed \n", "\n", "Scraping Page 46.\n", "828 listings scraped\n", "Page 46 completed \n", "\n", "Scraping Page 47.\n", "846 listings scraped\n", "Page 47 completed \n", "\n", "Scraping Page 48.\n", "864 listings scraped\n", "Page 48 completed \n", "\n", "Scraping Page 49.\n", "879 listings scraped\n", "Page 49 completed \n", "\n", "Scraping Page 50.\n", "No more listings left to scrape, pages scraped successfully 49\n", "Task Completed\n"]}], "source": ["Run_This_Function = get_restaurant_csv()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["***\n", "![Final Result](https://i.imgur.com/Fh3D2Co.png)\n", "\n", "You can find the output file in your jupyter directory."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Final Result\n", "\n", "Mounting restaurants.csv to a dataframe so it can be analyzed further."]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Reading the csv file and storing it in df dataframe.\n", "df = pd.read_csv('restaurants.csv')"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"scrolled": true}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>restaurant_names</th>\n", "      <th>restaurant_locations</th>\n", "      <th>costs_for_2</th>\n", "      <th>cusisines</th>\n", "      <th>ratings</th>\n", "      <th>restaurant_imgs</th>\n", "      <th>restaurant_pages</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>Farmers Basket At Pluck</td>\n", "      <td>Pullman New Delhi Aerocity-An AccorHotels Brand</td>\n", "      <td>\u20b9 3000</td>\n", "      <td>Chinese,European,Finger Food,North Indian,Mode...</td>\n", "      <td>4.5</td>\n", "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n", "      <td>https://www.eazydiner.com/delhi-ncr/pluck-pull...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Connaught Clubhouse</td>\n", "      <td>Janpath, Connaught Place (CP), Central Delhi</td>\n", "      <td>\u20b9 2000</td>\n", "      <td>Beverages,Finger Food,Italian,Mexican,Multicui...</td>\n", "      <td>4.4</td>\n", "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n", "      <td>https://www.eazydiner.com/delhi-ncr/connaught-...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>Clock Tower</td>\n", "      <td>Golf Course Road, Gurgaon</td>\n", "      <td>\u20b9 2700</td>\n", "      <td>Beverages,Finger Food,Italian,North Indian,Mul...</td>\n", "      <td>4.7</td>\n", "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n", "      <td>https://www.eazydiner.com/delhi-ncr/the-clock-...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>Moti Mahal Delux</td>\n", "      <td>South Extension 2, South Delhi</td>\n", "      <td>\u20b9 1400</td>\n", "      <td>Chinese,Mughlai,North Indian</td>\n", "      <td>5.0</td>\n", "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n", "      <td>https://www.eazydiner.com/delhi-ncr/moti-mahal...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>DRAMZ</td>\n", "      <td>Mehrauli, South Delhi</td>\n", "      <td>\u20b9 3000</td>\n", "      <td>European,Italian,North Indian,Cocktail Menu,Co...</td>\n", "      <td>4.1</td>\n", "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n", "      <td>https://www.eazydiner.com/delhi-ncr/dramz-whis...</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["          restaurant_names                             restaurant_locations  \\\n", "0  Farmers Basket At Pluck  Pullman New Delhi Aerocity-An AccorHotels Brand   \n", "1      Connaught Clubhouse     Janpath, Connaught Place (CP), Central Delhi   \n", "2              Clock Tower                        Golf Course Road, Gurgaon   \n", "3         Moti Mahal Delux                   South Extension 2, South Delhi   \n", "4                    DRAMZ                            Mehrauli, South Delhi   \n", "\n", "  costs_for_2                                          cusisines  ratings  \\\n", "0      \u20b9 3000  Chinese,European,Finger Food,North Indian,Mode...      4.5   \n", "1      \u20b9 2000  Beverages,Finger Food,Italian,Mexican,Multicui...      4.4   \n", "2      \u20b9 2700  Beverages,Finger Food,Italian,North Indian,Mul...      4.7   \n", "3      \u20b9 1400                       Chinese,Mughlai,North Indian      5.0   \n", "4      \u20b9 3000  European,Italian,North Indian,Cocktail Menu,Co...      4.1   \n", "\n", "                                     restaurant_imgs  \\\n", "0  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n", "1  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n", "2  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n", "3  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n", "4  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n", "\n", "                                    restaurant_pages  \n", "0  https://www.eazydiner.com/delhi-ncr/pluck-pull...  \n", "1  https://www.eazydiner.com/delhi-ncr/connaught-...  \n", "2  https://www.eazydiner.com/delhi-ncr/the-clock-...  \n", "3  https://www.eazydiner.com/delhi-ncr/moti-mahal...  \n", "4  https://www.eazydiner.com/delhi-ncr/dramz-whis...  "]}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": ["# Pring the top 5 rows of the data.\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- The Scraping was done using Python libraries Requests, BeautifulSoup for extracting the data and Pandas for exporting it.\n", "\n", "- Scrape multiple pages for Restaurant Name, Restaurant Location, Cost for two, Cuisines, Restaurant Rating, Restaurant Image, Link to restaurant page from any number of available pages.\n", "\n", "- Parsed all the scraped data into a .csv file containing total of 808 rows and 7 columns for each restaurant.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Future Updates\n", "\n", "- Add more option such as select city\n", "- Capture the reviews of these restaurants and perform analysis.\n", "- Code optimization.\n", "- Further documentation."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## References"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- [https://www.eazydiner.com/](https://www.eazydiner.com/)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/javascript": ["window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"], "text/plain": ["<IPython.core.display.Javascript object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["jovian.commit(files=['restaurants.csv'])"]}], "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.6"}}, "nbformat": 4, "nbformat_minor": 2}