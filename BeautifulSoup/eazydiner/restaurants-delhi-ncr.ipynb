{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Scraping Restaurants in Delhi From EazyDiner__\n",
    "\n",
    "***\n",
    "\n",
    "![](https://i.imgur.com/ZAl12g3.png)\n",
    "\n",
    "***\n",
    "\n",
    "[EazyDiner](https://www.eazydiner.com/static/about-us) provides a guide to eating out that offers insider tips, discount, exclusive and expert reviews by top critics. The platform has all the answers for the most enjoyable, authentic and friction-free table booking experience, with over 10,000 restaurants in over 150 cities in India & Dubai.\n",
    "\n",
    "***\n",
    "\n",
    "![Imgur](https://i.imgur.com/CC2FfCF.png)\n",
    "\n",
    "It also hosts list of top restaurants in a city and we will be using this list to scrape the top restaurants in the region [Delhi-NCR](https://www.eazydiner.com/restaurants?location=delhi-ncr&pax=2&total=281&page=1) using [_web scraping_](https://www.geeksforgeeks.org/what-is-web-scraping-and-how-to-use-it/), an automatic methord of obtaining large amounts of data from websites using coding languages such as `Python`,`C++`,`Node.js`,`Ruby`,`PHP` etc. \n",
    "\n",
    "For this project we will be using Python libraries [requests](https://pypi.org/project/requests/) and [Beautifulsoup4](https://pypi.org/project/beautifulsoup4/) for scraping data from this page.\n",
    "\n",
    "***\n",
    "\n",
    "#### Here is an outline of the steps we will follow\n",
    "\n",
    "##### 1. Setting up the environment\n",
    "##### 2.  Downloading the page using `requests` & parsing it with `BeautifulSoup` \n",
    "##### 3. Extracting restaurant information & appending to a dictionary\n",
    "##### 4. Compiling the data from multiple pages into a single file using lists and dictionaries\n",
    "##### 5. Exporting the data to a .CSV file\n",
    "\n",
    "***\n",
    "\n",
    "![Imgur](https://i.imgur.com/KwwD5KA.png)\n",
    "\n",
    "__By the end of the project we would have created a .CSV file in this format__\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "Use the \"Run\" button to execute the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "\n",
    "We will be using the `requests` ,`BeautifulSoup` and `pandas` libraries in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests will help us with fetching the HTML page from a website.\n",
    "\n",
    "import requests\n",
    "\n",
    "# Next,we will use BeautifulSoup to process the HTML formated text file for data extraction.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# After the data has been parsed and stored, we will use pandas to extract it into a '.csv' file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#\n",
    "import jovian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The environment is all set up now and we can call any function from these libraries.__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the page using `requests` & parsing it with `BeautifulSoup` \n",
    "\n",
    "Here we will define function get_page() that takes a URL as input and with the help of `requests` & `BeautifulSoup` returns a BS4 doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \n",
    "    # requests.get returns a response object containing the data from the web page.\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # status_code is used to check if the request was successful and if it's not then we will raise an exception.\n",
    "    if response.status_code != 200:\n",
    "        \n",
    "        # Exception will be raised if the status code is not 200\n",
    "        raise Exception (\"Unable to fetch page \" + url)\n",
    "    \n",
    "    # At the end of function it will return a beautifulsoup doc\n",
    "    return BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now the function can be called using `get_page(xyz.com)` and it will return a beautifulsoup doc.__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting restaurant information & Writing it in a dictionary\n",
    "\n",
    "##### Here we will extract the data from the restaurant listings & append it to a dictionary \n",
    "Restaurant Name, Restaurant Location, Cost for two, Cuisines, Restaurant Rating, Restaurant Image, Link to restaurant page\n",
    "\n",
    "\n",
    "***\n",
    "__Inspect Element Page__\n",
    "![Imgur](https://i.imgur.com/mNCLTg9.png)\n",
    "\n",
    "__For parsing data from a HTML page using beautiful soup we will need the [CSS selectors](https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors) of the elements, to get these selectors we will use the [inspect element](https://devmountain.com/blog/how-to-use-inspect-element-jump-into-what-makes-a-web-page-tick/) function available in the web browser.__\n",
    "\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant Listings\n",
    "\n",
    "![Imgur](https://i.imgur.com/5zO4cT6.png)\n",
    "\n",
    "The restaurant listings are inside a `div` tag with `padding-10 radius-4 bg-white restaurant margin-b-10` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurant_listings(doc):\n",
    "    ''''''\n",
    "    # Declaring a variable selector that contains class for name tag.\n",
    "    selector = 'padding-10 radius-4 bg-white restaurant margin-b-10'\n",
    "    \n",
    "    # Returning the restaurant lising tags\n",
    "    return  doc.find_all('div',class_=selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Restaurant Details\n",
    "\n",
    "Similarly by using the inspect element we can find tags for all the fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "`restaurant_name = listing.find('h3',class_='grey res_name font-20 bold inline-block')`\n",
    "\n",
    "`restaurant_location = listing.find('h3',class_='margin-t-5 res_loc')`\n",
    "\n",
    "`cost_for_2 = listing.find('span',class_='padding-l-10 grey cost_for_two')`\n",
    "\n",
    "`cusisine = listing.find('div',class_='grey padding-l-10 res_cuisine')`\n",
    "\n",
    "`rating = listing.find('span',class_='critic')`\n",
    "\n",
    "`restaurant_img = listing.find('img',class_='radius-4 res_name lazy')`\n",
    "\n",
    "`restaurant_href = listing.find('a',class_='btn btn-primary height-40 block bold padding-10 font-14 apxor_click')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurant_data(listing,info,base_url):\n",
    "    \n",
    "    # Parsing the information from lising\n",
    "    restaurant_name = listing.find('h3',class_='grey res_name font-20 bold inline-block')\n",
    "    restaurant_location = listing.find('h3',class_='margin-t-5 res_loc')\n",
    "    cost_for_2 = listing.find('span',class_='padding-l-10 grey cost_for_two')\n",
    "    cusisine = listing.find('div',class_='grey padding-l-10 res_cuisine')\n",
    "    rating = listing.find('span',class_='critic')\n",
    "    restaurant_img = listing.find('img',class_='radius-4 res_name lazy')\n",
    "    restaurant_href = listing.find('a',class_='btn btn-primary height-40 block bold padding-10 font-14 apxor_click')\n",
    "\n",
    "    # Appending the extracted info to dictionary\n",
    "    info['restaurant_names'].append(restaurant_name.text.strip() if restaurant_name else 'N/A')\n",
    "    info['restaurant_locations'].append(restaurant_location.text.strip() if restaurant_location else 'N/A')\n",
    "    info['costs_for_2'].append(cost_for_2.text[:-7] if cost_for_2 else 'N/A')\n",
    "    info['cusisines'].append(cusisine.text.strip() if cusisine else 'N/A')\n",
    "    info['ratings'].append(rating.text.strip() if rating else 'N/A')\n",
    "    info['restaurant_imgs'].append(restaurant_img['data-src'] if restaurant_img else 'N/A')\n",
    "    info['restaurant_pages'].append(base_url+restaurant_href['href'] if restaurant_href else 'N/A')\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We have defined 2 functions `get_restaurant_listings` & `get_restaurant_data` , first for getting listing tags form the page and second for extracting data from the listing and appending it to a dictionary.__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the data from multiple pages into a single file using lists and dictionaries.\n",
    "\n",
    "`intialize_dictionary()` will be used create a empty dictionary for every the data is scraped\n",
    "\n",
    "`page_parser()` to get the data from every listing on the page\n",
    "\n",
    "`website_scraper()` to get every page on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_dictionary():\n",
    "    \n",
    "    # Intitalizing a new dictionary for stong the values\n",
    "    info = {\n",
    "        'restaurant_names':[],\n",
    "        'restaurant_locations':[],\n",
    "        'costs_for_2':[],\n",
    "        'cusisines':[],\n",
    "        'ratings':[],\n",
    "        'restaurant_imgs': [],\n",
    "        'restaurant_pages':[]\n",
    "    }\n",
    "              \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def page_parser(x,max_page,website_page_base_url,info,base_url):\n",
    "    \n",
    "    while True: # Keep the loop running till a break condition is met\n",
    "        \n",
    "        # Adding if condition to stop the function if the requested number of pages have been scraped\n",
    "        if x > int(max_page):         \n",
    "            print(\"Process completed!, No more data to scrape after page {}\".format(x-1)) # printing a confirmation message\n",
    "            break\n",
    "        \n",
    "        page_url = website_page_base_url + str(x) # Creating page url\n",
    "        print(\"Scraping Page {}.\".format(x))  # printing a confirmation message\n",
    "        doc = get_page(page_url) # Calling function to get bs4 doc\n",
    "        doc_listings = get_restaurant_listings(doc) # geting all the listing on the page\n",
    "        \n",
    "        if len(doc_listings) < 1: # Stop the loop if no data to scrape\n",
    "            print(\"No more listings left to scrape, pages scraped successfully {}\".format(x-1)) # printing a confirmation message\n",
    "            break\n",
    "        \n",
    "        # Starting a for loop to get data from page\n",
    "        for listing in doc_listings:           \n",
    "            info = get_restaurant_data(listing,info,base_url) # Extracting data from all listing using a for loop\n",
    "\n",
    "        print(\"{} listings scraped\".format(len(info['restaurant_names']))) # printing a confirmation \n",
    "        print(\"Page {} completed \\n\".format(x)) # printing a confirmation \n",
    "        \n",
    "        # Increasing the page number\n",
    "        x = x + 1\n",
    "        \n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def website_scraper():\n",
    "    \n",
    "    # Assigning values to variables that will be used in the function\n",
    "    x = 1 # x is the starting page number\n",
    "    base_url = \"https://www.eazydiner.com\"  \n",
    "    website_page_base_url = base_url + '/restaurants?location=delhi-ncr&pax=2&total=281&page='\n",
    "    \n",
    "    # Intitalizing a new dictionary for stong the values\n",
    "    info = intialize_dictionary()\n",
    "    print(\"Initializing a new database \\n\")\n",
    "        \n",
    "    # Asking user for input\n",
    "    max_page = input(\"Please enter the number of pages you want to scrape: \")\n",
    "    \n",
    "    # Calling previously defined function to get the data from the page\n",
    "    info = page_parser(x,max_page,website_page_base_url,info,base_url)\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the data to a .CSV file\n",
    "\n",
    "After parsing the data from the web page and storing it in a dictionary , we will use `pandas` to export it to a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restaurant_csv():\n",
    "    \n",
    "    # Variable ourput calls the function get_restaurants() and stores it's value inside it.\n",
    "    output = website_scraper()\n",
    "    \n",
    "    # Convering the dictionary outpt to a pandas dataframe\n",
    "    df = pd.DataFrame(output)\n",
    "    \n",
    "    # Adding current date \n",
    "    \n",
    "    # returning restaurants.csv file\n",
    "    return df.to_csv('restaurants.csv', index=None),print(\"Task Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing a new database \n",
      "\n",
      "Please enter the number of pages you want to scrape: 3\n",
      "Scraping Page 1.\n",
      "18 listings scraped\n",
      "Page 1 completed \n",
      "\n",
      "Scraping Page 2.\n",
      "36 listings scraped\n",
      "Page 2 completed \n",
      "\n",
      "Scraping Page 3.\n",
      "52 listings scraped\n",
      "Page 3 completed \n",
      "\n",
      "Process completed!, No more data to scrape after page 3\n",
      "Task Completed\n"
     ]
    }
   ],
   "source": [
    "Run_This_Function = get_restaurant_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "![Final Result](https://i.imgur.com/Fh3D2Co.png)\n",
    "\n",
    "You can find the output file in your jupyter directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result\n",
    "\n",
    "Mounting restaurants.csv to a dataframe so it can be analyzed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file and storing it in df dataframe.\n",
    "df = pd.read_csv('restaurants.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_names</th>\n",
       "      <th>restaurant_locations</th>\n",
       "      <th>costs_for_2</th>\n",
       "      <th>cusisines</th>\n",
       "      <th>ratings</th>\n",
       "      <th>restaurant_imgs</th>\n",
       "      <th>restaurant_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lord of the Drinks</td>\n",
       "      <td>Connaught Place (CP), Central Delhi</td>\n",
       "      <td>₹ 1800</td>\n",
       "      <td>Chinese,European,Finger Food,Italian,North Ind...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n",
       "      <td>https://www.eazydiner.com/delhi-ncr/lord-of-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fifty9</td>\n",
       "      <td>Radisson Blu Marina, New Delhi</td>\n",
       "      <td>₹ 2500</td>\n",
       "      <td>Multicuisine</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n",
       "      <td>https://www.eazydiner.com/delhi-ncr/fifty9-rad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kinbuck 2 Cafe &amp; Bar</td>\n",
       "      <td>Connaught Place (CP), Central Delhi</td>\n",
       "      <td>₹ 1200</td>\n",
       "      <td>Chinese,Italian,Lebanese,Mexican,North Indian</td>\n",
       "      <td>4.2</td>\n",
       "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n",
       "      <td>https://www.eazydiner.com/delhi-ncr/kinbuck-2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mist</td>\n",
       "      <td>The Park, New Delhi</td>\n",
       "      <td>₹ 2100</td>\n",
       "      <td>Casual Eclectic</td>\n",
       "      <td>4.6</td>\n",
       "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n",
       "      <td>https://www.eazydiner.com/delhi-ncr/mist-the-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pind Balluchi</td>\n",
       "      <td>Netaji Subhash Place, North Delhi</td>\n",
       "      <td>₹ 1000</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://d4t7t8y8xqo0t.cloudfront.net/resized/1...</td>\n",
       "      <td>https://www.eazydiner.com/delhi-ncr/pind-ballu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       restaurant_names                 restaurant_locations costs_for_2  \\\n",
       "0    Lord of the Drinks  Connaught Place (CP), Central Delhi      ₹ 1800   \n",
       "1                Fifty9       Radisson Blu Marina, New Delhi      ₹ 2500   \n",
       "2  Kinbuck 2 Cafe & Bar  Connaught Place (CP), Central Delhi      ₹ 1200   \n",
       "3                  Mist                  The Park, New Delhi      ₹ 2100   \n",
       "4         Pind Balluchi    Netaji Subhash Place, North Delhi      ₹ 1000   \n",
       "\n",
       "                                           cusisines  ratings  \\\n",
       "0  Chinese,European,Finger Food,Italian,North Ind...      4.2   \n",
       "1                                       Multicuisine      4.2   \n",
       "2      Chinese,Italian,Lebanese,Mexican,North Indian      4.2   \n",
       "3                                    Casual Eclectic      4.6   \n",
       "4                                       North Indian      4.1   \n",
       "\n",
       "                                     restaurant_imgs  \\\n",
       "0  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n",
       "1  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n",
       "2  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n",
       "3  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n",
       "4  https://d4t7t8y8xqo0t.cloudfront.net/resized/1...   \n",
       "\n",
       "                                    restaurant_pages  \n",
       "0  https://www.eazydiner.com/delhi-ncr/lord-of-th...  \n",
       "1  https://www.eazydiner.com/delhi-ncr/fifty9-rad...  \n",
       "2  https://www.eazydiner.com/delhi-ncr/kinbuck-2-...  \n",
       "3  https://www.eazydiner.com/delhi-ncr/mist-the-p...  \n",
       "4  https://www.eazydiner.com/delhi-ncr/pind-ballu...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pring the top 5 rows of the data.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Scraping was done using Python libraries Requests, BeautifulSoup for extracting the data and Pandas for exporting it.\n",
    "\n",
    "- Scrape multiple pages for Restaurant Name, Restaurant Location, Cost for two, Cuisines, Restaurant Rating, Restaurant Image, Link to restaurant page from any number of available pages.\n",
    "\n",
    "- Parsed all the scraped data into a .csv file containing total of 284 rows and 7 columns for each restaurant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Updates\n",
    "\n",
    "- Add more option such as select city\n",
    "- Capture the reviews of these restaurants and perform analysis.\n",
    "- Code optimization.\n",
    "- Further documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [https://www.eazydiner.com/](https://www.eazydiner.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "jovian.commit(files=['restaurants.csv'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}