{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b74a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Install libraries that will be crutial for web scraping\n",
    "'''\n",
    "\n",
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3502eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 1: Requests will make request to the server.\n",
    "\n",
    "Step 2: BeautifulSoup will be used to parse though the html reponse we will get\n",
    "        by using requests.\n",
    "        \n",
    "Step 3: After we have parsed the html file and collcted the desired data in a\n",
    "        a dictionary we will use csv module to expore data in a .csv file.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d3be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def github_repo_scraper():\n",
    "    \n",
    "    '''\n",
    "    This function returns top github repositores of any given topic \n",
    "    in a step by step process\n",
    "    1.First we ask the user to provide a topic name\n",
    "    2.Then we fetch the topic page using fetch_page()\n",
    "    3.After we have the page we will tags_parser() to get list of all the tags containing repository data\n",
    "    4.Now we will use repo_parser() to parse the repositories from all the tags\n",
    "    5.After we have all the data write_repo_csv() will convert the dictionary output to .csv file\n",
    "    '''\n",
    "    \n",
    "    topic = input(\"Please enter a topic :\")\n",
    "    \n",
    "    doc = fetch_page('topic')\n",
    "    \n",
    "    doc_tags = tags_parser(doc)\n",
    "    \n",
    "    top_repositories = [repo_parser(tag) for tag in doc_tags]\n",
    "    \n",
    "    write_repo_csv(top_repositories,topic)\n",
    "    \n",
    "    return print(\"Top repositories saved as .csv in directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a6c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the fucntions called  by github_repo_scraper()\n",
    "\n",
    "def fetch_page(topic):\n",
    "    \n",
    "    '''\n",
    "    fetch_page() is a funtion that takes a topic name as input and return a beautful soup parsed file for the topic,\n",
    "    status_code is used here to check if we got a valid response from our request\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://github.com/topics/'\n",
    "    topic_url = base_url + topic\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Not able to fetch page \",topic_url,\" please check the input again\")\n",
    "    return BeautifulSoup(response.text)    \n",
    "\n",
    "def tags_parser(doc):\n",
    "    \n",
    "    '''tags_parser() fetches the articles that contain repository data and return it in a list that will be itterated'''\n",
    "    \n",
    "    return doc.find_all('article',class_='border rounded color-shadow-small color-bg-subtle my-4') \n",
    "\n",
    "def repo_parser(tag):\n",
    "    \n",
    "    '''\n",
    "    repo_parser() will goes though the tags and return the owner_name,repo_name,repo_url and repo_stars in a dictionary\n",
    "    '''\n",
    "    \n",
    "    owner_name = tag.find('h3').find_all('a')[0].text.strip()\n",
    "    \n",
    "    repo_name = tag.find('h3').find_all('a')[1].text.strip()\n",
    "    \n",
    "    repo_tag = tag.find('h3').find_all('a')[1].attrs['href'].strip()\n",
    "    repo_url = 'https://github.com'+repo_tag\n",
    "    \n",
    "    star_string = tag.find('span',id='repo-stars-counter-star').text.strip()\n",
    "    repo_stars = parse_star(star_string)\n",
    "    \n",
    "    return{\n",
    "        'repository_name':repo_name,\n",
    "        'owner_username':owner_name,\n",
    "        'stars':repo_stars,\n",
    "        'repository_url':repo_url\n",
    "    }\n",
    "\n",
    "def parse_star(star_string):\n",
    "    \n",
    "    '''\n",
    "    parse_star() takes the sting value fo stars as input and return an integer star count\n",
    "    '''\n",
    "    \n",
    "    if star_string[-1] == 'k':\n",
    "        return int(float(star_string[:-1])*1000)\n",
    "    else: return int(star_string)\n",
    "    \n",
    "def write_repo_csv(dic_output,topic): \n",
    "    \n",
    "    '''\n",
    "    using the csv module write_repo_csv() return a .csv file after taking the dictionary of repositories as input\n",
    "    '''\n",
    "    \n",
    "    headers = list(dic_output[1].keys())\n",
    "    file_name = topic+\"_repo.csv\"\n",
    "    with open(file_name,'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile,fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dic_output)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc9a19c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a topic :python\n",
      "Top repositories saved as .csv in directory\n"
     ]
    }
   ],
   "source": [
    "github_repo_scraper()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
